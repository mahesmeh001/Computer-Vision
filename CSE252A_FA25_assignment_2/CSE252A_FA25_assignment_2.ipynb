{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7ef85hA0Wyp"
      },
      "source": [
        "# CSE 252A Computer Vision I Fall 2025 - Assignment 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFW0v-BaqZzL"
      },
      "source": [
        "Instructor: Ben Ochoa\n",
        "\n",
        "Assigment due: Wed, Nov 5, 11:59 PM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5VjlvfwqZzL"
      },
      "source": [
        "**Name:**\n",
        "\n",
        "**PID:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj5Sa0CNqZzM"
      },
      "source": [
        "## Instructions\n",
        "\n",
        "Please answer the questions below using Python in the attached Jupyter notebook and follow the guidelines below:\n",
        "\n",
        "- This assignment must be completed **individually**. For more details, please follow the Academic Integrity Policy and Collaboration Policy on [Canvas](https://canvas.ucsd.edu).\n",
        "\n",
        "- All the solutions must be written in this Jupyter notebook.\n",
        "\n",
        "- You may use basic algebra packages (e.g. `NumPy`, `SciPy`, etc) but you are not allowed to use the packages that directly solve the problems. Feel free to ask the instructor and the teaching assistants if you are unsure about the packages to use.\n",
        "\n",
        "- It is highly recommended that you begin working on this assignment early.\n",
        "\n",
        "- You must **submit 3 files: the Notebook, the PDF and the python file** (i.e. the `.ipynb`, the `.pdf` and the `.py` files) on Gradescope. **You must mark each problem on Gradescope in the pdf.**\n",
        "    - To convert the notebook to PDF, you can choose one way below:\n",
        "\n",
        "        - You may first export the notebook as HTML, and then print the web page as PDF\n",
        "\n",
        "            - e.g., in Chrome: File $\\rightarrow$ Save and Export Notebook as $\\rightarrow$ \"HTML\"; or in VScode: Open the Command Palette by pressing Ctrl+Shift+P (Windows/Linux) or Cmd+Shift+P (macOS), search for Jupyter: Export to HTML\n",
        "    \n",
        "            - Open the saved web page and right click $\\rightarrow$ Print... $\\rightarrow$ Choose \"Destination: Save as PDF\" and click \"Save\")\n",
        "\n",
        "        - If you have XeTex installed on your machine, you may directly export the notebook as PDF: e.g., in Chrome, File $\\rightarrow$ Save and Export Notebook as $\\rightarrow$ \"PDF\"\n",
        "\n",
        "        - You may use [nbconvert](https://nbconvert.readthedocs.io/en/latest/install.html) to convert the ipynb file to pdf using the following command\n",
        "        `jupyter nbconvert --allow-chromium-download --to webpdf filename.ipynb`\n",
        "\n",
        "    - To convert the notebook to python file, you can choose one way below:\n",
        "\n",
        "        - You may directly export the notebook as py: e.g., in Chrome, File $\\rightarrow$ Save and Export Notebook as $\\rightarrow$ \"Executable script\"; or in VScode: Open the Command Palette and search for Jupyter: Export to Python Script\n",
        "\n",
        "        - You may use [nbconvert](https://nbconvert.readthedocs.io/en/latest/install.html) to convert the ipynb file to python file using the following command\n",
        "    `jupyter nbconvert --to script filename.ipynb --output output_filename.py`\n",
        "\n",
        "- Please make sure the content in each cell (e.g. code, output images, printed results, etc.) are clearly visible and are not cut-out or partially cropped in your final PDF file.\n",
        "\n",
        "- While submitting on gradescope, please make sure to assign the relevant pages in your PDF submission for each problem.\n",
        "\n",
        "**Late Policy:** Assignments submitted late will receive a 15% grade reduction for each 12 hours late (i.e., 30% per day). Assignments will not be accepted 72 hours after the due date. If you require an extension (for personal reasons only) to a due date, you must request one as far in advance as possible. Extensions requested close to or after the due date will only be granted for clear emergencies or clearly unforeseeable circumstances."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cm3FG3yz0Wyv"
      },
      "source": [
        "## Problem 1 Image filtering [15 pts]\n",
        "\n",
        "### Problem 1.1 Implementing Convolution [5 pts]\n",
        "\n",
        "\n",
        "In this problem, you will implement the convolution filtering operation using NumPy functions, but without using the NumPy convolve function directly.\n",
        "\n",
        "As shown in the lecture, a convolution can be considered as a sliding window that computes a sum of the pixel values weighted by the rotated kernel. Your version will\n",
        "- zero-pad an image,\n",
        "- rotate the kernel $180^\\circ$, and\n",
        "- compute a weighted sum of the neighborhood at each pixel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKng2qVghPQw"
      },
      "source": [
        "#### Problem 1.1.1  [1 pts]\n",
        "First you will want to implement the `zero_pad` function. Add padding to the given cat image by extending it 50 pixels on the top and bottom and 30 pixels on the left and right."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CXSBOp31DgyB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from time import time\n",
        "from skimage import io\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGaGnlrrDWLa",
        "outputId": "388658e6-0b14-4eff-abf0-d59b757da1b2"
      },
      "outputs": [],
      "source": [
        "def zero_pad(image, pad_top, pad_down, pad_left, pad_right):\n",
        "    \"\"\"\n",
        "    Zero-pad an image.\n",
        "\n",
        "    Ex: a 1x1 image [[1]] with pad_top = 1, pad_down = 1, pad_left = 2, pad_right = 2 becomes:\n",
        "\n",
        "        [[0, 0, 0, 0, 0],\n",
        "         [0, 0, 1, 0, 0],\n",
        "         [0, 0, 0, 0, 0]]\n",
        "\n",
        "    of shape (3, 5)\n",
        "\n",
        "    Args:\n",
        "        image: numpy array of shape (H, W)\n",
        "        pad_left: width of the zero padding to the left of the first column\n",
        "        pad_right: width of the zero padding to the right of the last column\n",
        "        pad_top: height of the zero padding above the first row\n",
        "        pad_down: height of the zero padding below the last row\n",
        "\n",
        "    Returns:\n",
        "        out: numpy array of shape (H + pad_top + pad_down, W + pad_left + pad_right)\n",
        "    \"\"\"\n",
        "    \"\"\" ==========\n",
        "    YOUR CODE HERE\n",
        "    ========== \"\"\"\n",
        "    \n",
        "    return out\n",
        "\n",
        "# Open image as grayscale\n",
        "img = io.imread('cat.jpg', as_gray=True)\n",
        "\n",
        "# Show image\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.imshow(img,cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "\"\"\" ==========\n",
        "YOUR CODE HERE TO PAD THE IMAGE\n",
        "========== \"\"\"\n",
        "padded_img = \n",
        "\n",
        "# Plot your padded cat\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(padded_img,cmap='gray')\n",
        "plt.title('Padded cat')\n",
        "plt.axis('off')\n",
        "\n",
        "# Plot what you should get\n",
        "solution_img = io.imread('padded_cat.png', as_gray=True)\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(solution_img,cmap='gray')\n",
        "plt.title('What you should get')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU_JWFjQhPQ3"
      },
      "source": [
        "#### Problem 1.1.2 [2 pts]\n",
        "Now implement the function **`conv`**, **using at most 2 loops**. This function takes an image $f$ and a kernel $h$ as inputs and output the convolved image $(f*h)$ that has the same shape as the input image (use zero padding before convolution to accomplish this). We will only be using kernels with odd width and odd height. Depending on the computer, your implementation should take around a second or less to run. **Do NOT use existing library functions that directly perform convolution.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEq2Z4W_hPQ4",
        "outputId": "6e9ea1a1-b838-4acc-cab9-320533825eb5"
      },
      "outputs": [],
      "source": [
        "def conv(image, kernel):\n",
        "    \"\"\"\n",
        "    An efficient implementation of a convolution filter.\n",
        "\n",
        "    This function uses element-wise multiplication and np.sum()\n",
        "    to efficiently compute a weighted sum of the neighborhood at each\n",
        "    pixel.\n",
        "\n",
        "    Hints:\n",
        "        - Use the zero_pad function you implemented above\n",
        "        - Use at most two nested for-loops\n",
        "        - You may find np.flip() and np.sum() useful\n",
        "        - You need to handle both odd and even kernel size\n",
        "\n",
        "    Args:\n",
        "        image: numpy array of shape (Hi, Wi)\n",
        "        kernel: numpy array of shape (Hk, Wk)\n",
        "\n",
        "    Returns:\n",
        "        out: numpy array of shape (Hi, Wi)\n",
        "    \"\"\"\n",
        "    Hi, Wi = image.shape\n",
        "    Hk, Wk = kernel.shape\n",
        "    out = np.zeros((Hi, Wi))\n",
        "\n",
        "    \"\"\" ==========\n",
        "    YOUR CODE HERE\n",
        "    ========== \"\"\"\n",
        "    \n",
        "\n",
        "    return out\n",
        "\n",
        "# Simple convolution kernel.\n",
        "kernel = np.array(\n",
        "[\n",
        "    [1,0,-1],\n",
        "    [2,0,-2],\n",
        "    [1,0,-1]\n",
        "])\n",
        "\n",
        "t1 = time()\n",
        "out = conv(img, kernel)\n",
        "t2 = time()\n",
        "print(\"took %f seconds.\" % (t2 - t1))\n",
        "\n",
        "# Plot original image\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.subplot(2,2,1)\n",
        "plt.imshow(img,cmap='gray')\n",
        "plt.title('Original')\n",
        "plt.axis('off')\n",
        "\n",
        "# Plot your convolved image\n",
        "plt.subplot(2,2,3)\n",
        "plt.imshow(out,cmap='gray')\n",
        "\n",
        "plt.title('Convolution')\n",
        "plt.axis('off')\n",
        "\n",
        "# Plot what you should get\n",
        "solution_img = io.imread('convolved_cat.png', as_gray=True)\n",
        "plt.subplot(2,2,4)\n",
        "plt.imshow(solution_img,cmap='gray')\n",
        "plt.title('What you should get')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nN7ijOi4hPQ6"
      },
      "source": [
        "#### Problem 1.1.3 [1 pt]\n",
        "Now let's filter some images! Here, you will apply the convolution function that you just implemented in order to bring about some interesting image effects. More specifically, we will use convolution to blur and sharpen our images.\n",
        "\n",
        "First we will apply convolution for image blurring. To accomplish this, convolve the cat image with a 25x25 Gaussian filter for $\\sigma = 4$. You can use the included function to obtain the Gaussian kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0C__v-mhPQ7",
        "outputId": "c9f2a825-37fc-41be-a63c-5cf179cbe2a8"
      },
      "outputs": [],
      "source": [
        "def gaussian2d(sig):\n",
        "    \"\"\"\n",
        "    Creates 2D Gaussian kernel with a sigma of `sig`.\n",
        "    Arguments:\n",
        "        sig: Standard deviation of the Gaussian smoothing kernel.\n",
        "    Returns:\n",
        "        kernel: 2D Gaussian Kernel\n",
        "    \"\"\"\n",
        "    filter_size = int(sig * 6)\n",
        "    if filter_size % 2 == 0:\n",
        "        filter_size += 1\n",
        "\n",
        "    ax = np.arange(-filter_size // 2 + 1., filter_size // 2 + 1.)\n",
        "    xx, yy = np.meshgrid(ax, ax)\n",
        "    kernel = np.exp(-0.5 * (np.square(xx) + np.square(yy)) / np.square(sig))\n",
        "    return kernel / np.sum(kernel)\n",
        "\n",
        "def blur_image(img, sigma):\n",
        "    \"\"\"\n",
        "    Blur the image by convolving with a Gaussian filter.\n",
        "    Arguments:\n",
        "        img: numpy array of shape (H, W)\n",
        "        sigma: standard deviation of the Gaussian filter\n",
        "    Returns:\n",
        "        blurred_img: numpy array of shape (H, W)\n",
        "    \"\"\"\n",
        "    blurred_img = np.zeros_like(img)\n",
        "    \"\"\" ==========\n",
        "    YOUR CODE HERE\n",
        "    ========== \"\"\"\n",
        "    \n",
        "    return blurred_img\n",
        "\n",
        "# Plot original image and blurred image\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.subplot(2,2,1)\n",
        "plt.imshow(img,cmap='gray')\n",
        "plt.title('Original')\n",
        "plt.axis('off')\n",
        "\n",
        "\n",
        "\"\"\" ==========\n",
        "YOUR CODE HERE TO COMPUTE BLURRED IMAGE\n",
        "========== \"\"\"\n",
        "blurred_cat =\n",
        "\n",
        "plt.subplot(2,2,2)\n",
        "plt.imshow(blurred_cat,cmap='gray')\n",
        "plt.title('Blurred')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IuxOlr_hPQ8"
      },
      "source": [
        "#### Problem 1.1.4 [1 pt]\n",
        "Next, we will use convolution to sharpen the images. Convolve the image with the following filter to produce a sharpened result. For convenience, we have defined the filter for you:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuQCRtcShPQ9",
        "outputId": "b91b29e6-233b-4971-bc7e-a625eba0ca97"
      },
      "outputs": [],
      "source": [
        "def sharpen_image(img):\n",
        "    \"\"\"\n",
        "    Sharpen the image by convolving with a sharpening filter.\n",
        "    Arguments:\n",
        "        img: numpy array of shape (H, W)\n",
        "    Returns:\n",
        "        sharpened_img: numpy array of shape (H, W)\n",
        "    \"\"\"\n",
        "    sharpened_img = np.zeros_like(img)\n",
        "    # This is a 5x5 kernel used for sharpening an image.\n",
        "    sharpening_kernel = np.array([\n",
        "        [1, 4,     6,  4, 1],\n",
        "        [4, 16,   24, 16, 4],\n",
        "        [6, 24, -476, 24, 6],\n",
        "        [4, 16,   24, 16, 4],\n",
        "        [1,  4,    6,  4, 1],\n",
        "    ]) * -1.0 / 256.0\n",
        "\n",
        "    \"\"\" ==========\n",
        "    YOUR CODE HERE\n",
        "    ========== \"\"\"\n",
        "    \n",
        "    return sharpened_img\n",
        "\n",
        "# Plot original image\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.subplot(2,2,1)\n",
        "plt.imshow(img, vmin=0.0, vmax=1.0,cmap='gray')\n",
        "plt.title('Original')\n",
        "plt.axis('off')\n",
        "\n",
        "# Plot sharpened image\n",
        "plt.subplot(2,2,2)\n",
        "plt.imshow(sharpen_image(img), vmin=0.0, vmax=1.0,cmap='gray')\n",
        "plt.title('Sharpened')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 1.2: Nonlinear Filtering  [5 pts]\n",
        "#### Problem 1.2.1 [3 pts]\n",
        "\n",
        "Next, weâ€™ll apply a nonlinear filtering method to eliminate a particular type noise in the corrupted MRI image. The non corrupted MRI brain scan appears to have been transmitted over a noisy channel, causing some pixels to abruptly take on extreme values (black or white). You must decide on an appropriate filter (discussed within the scope of the lecture material) to remove this noise and determine an appropriate kernel size (trial and error basis) that removes it effectively. Also ensure the final filtered image is of the same size as the input image by using zero padding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def filter_mri_image(img, filter_size):\n",
        "    \"\"\"\n",
        "    Filter the image by applying a nonlinear filter.\n",
        "    Arguments:\n",
        "        img: numpy array of shape (H, W)\n",
        "        filter_size: size of the filter\n",
        "    Returns:\n",
        "        filtered_img: numpy array of shape (H, W)\n",
        "    \"\"\"\n",
        "    filtered_img = np.zeros_like(img)\n",
        "    \"\"\" ==========\n",
        "    YOUR CODE HERE\n",
        "    ========== \"\"\"\n",
        "\n",
        "\n",
        "    return filtered_img\n",
        "\n",
        "\n",
        "img = io.imread('mri_img.png', as_gray=True)\n",
        "# Plot original image\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.subplot(2,2,1)\n",
        "plt.imshow(img, vmin=0.0, vmax=1.0,cmap='gray')\n",
        "plt.title('Original')\n",
        "plt.axis('off')\n",
        "\n",
        "# Filter and plot the image\n",
        "\"\"\" ==========\n",
        "YOUR CODE HERE TO COMPUTE FILTERED IMAGE\n",
        "========== \"\"\"\n",
        "filtered_img = \n",
        "\n",
        "plt.subplot(2,2,2)\n",
        "plt.imshow(filtered_img, vmin=0.0, vmax=1.0,cmap='gray')\n",
        "plt.title('Filtered Image')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Problem 1.2.2 [2 pts]\n",
        "\n",
        "Would there be any artifacts due to zero padding (for same size output image as input) if we operated on the negative image (intensity inversed image) ? What can be done to resolve it ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwMFv2RihPQ-"
      },
      "source": [
        "### Problem 1.3: Convolution Theory  [5 pts]\n",
        "#### Problem 1.3.1 [2 pts]\n",
        "Consider (1) smoothing an image with a 3x3 averaging filter and then computing the derivative in the y-direction. Also consider (2) computing the derivative first, then smoothing. What is a single **convolution** kernel that will simultaneously implement both (1) and (2)? Try to give a brief justification for how you arrived at the kernel. (Hint: See shape full convolution)\n",
        "\n",
        "Use the y-derivative filter (**convolution**) for this problem.\n",
        "\n",
        "$\\begin{bmatrix}\n",
        "\\frac{1}{2} \\\\\n",
        "0 \\\\\n",
        "-\\frac{1}{2}\n",
        "\\end{bmatrix}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRJuBnacqZzP"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgSKAHAfhPQ-"
      },
      "source": [
        "#### Problem 1.3.2 [3 pts]\n",
        "Certain 2D filters can be expressed as a convolution of two 1D filters. Such filters are called separable filters. Give an example of a 3 $\\times$ 3 separable filter and compare the number of arithmetic operations it takes to\n",
        "convolve an n $\\times$ n image using that filter before and after separation. Count both, the number of multiplication and addition operations in each case.\n",
        "\n",
        "Assume that the convolution of the image and filter is performed in \"same\" mode, i.e., the input image is padded so that the output image has same dimensions as original input image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBOE0bMZqZzP"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cHhtDO8hPQ-"
      },
      "source": [
        "### Problem 1.4 Template Matching [5 pts]\n",
        "Suppose your company has several products of printed circuit boards (PCBs), and a specific component needs to be updated to its latest version. Right now, finding the location of this component on each PCB is a slow and manual task. To speed this up and avoid human error, you want to build a computer vision solution that can automatically locate that component using template matching.\n",
        "\n",
        "Luckily, you have learned in CSE 252A (or are learning right now) that convolution can be used for template matching: a rotated template $g$ is multiplied with regions of a larger image $f$ to measure how similar each region is to the template. Note that you will want to rotate the filter $180^\\circ$ before giving it to your convolution function, so that it is overall not rotated when making comparisons. You will also want to subtract off the mean value of the image or template (whichever you choose, subtract the same value from both the image and template) so that our solution is not biased toward higher-intensity (white) regions.\n",
        "\n",
        "The template of the component (template.png) and the image of one of the PCB (circuit_board.png) is provided. We will use convolution to find the component in the given PCB.\n",
        "\n",
        "<img src=\"template.png\" alt=\"template\" width=\"25px\"/>\n",
        "<img src=\"circuit_board.png\" alt=\"shelf\" width=\"600px\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FE-VFTvNhPQ_",
        "outputId": "6a4f4d6f-805f-4bd3-eced-45539e5c6a51"
      },
      "outputs": [],
      "source": [
        "# Load template and image in grayscale\n",
        "img = io.imread('circuit_board.png')\n",
        "img_gray = io.imread('circuit_board.png', as_gray=True)\n",
        "temp = io.imread('template.png')\n",
        "temp_gray = io.imread('template.png', as_gray=True)\n",
        "\n",
        "# Perform a convolution between the image (grayscale) and the template (grayscale) and store\n",
        "# the result in the out variable\n",
        "\"\"\" ==========\n",
        "YOUR CODE HERE\n",
        "========== \"\"\"\n",
        "\n",
        "\n",
        "# Display component template\n",
        "plt.figure(figsize=(20,16))\n",
        "plt.subplot(3, 1, 1)\n",
        "plt.imshow(temp_gray, cmap=\"gray\")\n",
        "plt.title('Template')\n",
        "plt.axis('off')\n",
        "\n",
        "# Display convolution output\n",
        "plt.subplot(3, 1, 2)\n",
        "plt.imshow(out, cmap=\"gray\")\n",
        "plt.title('Convolution output (white means more correlated)')\n",
        "plt.axis('off')\n",
        "\n",
        "# Display image\n",
        "plt.subplot(3, 1, 3)\n",
        "plt.imshow(img, cmap=\"gray\")\n",
        "plt.title('Result (red marker on the detected location)')\n",
        "plt.axis('off')\n",
        "\n",
        "# Draw marker at detected location\n",
        "plt.plot(x, y, 'rx', ms=15, mew=3) \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwqFRgPYhPRA"
      },
      "source": [
        "## Problem 2: Edge detection [21 pts]\n",
        "\n",
        "In this problem, you will write a function to perform Canny edge detection. You will use the image `ucsd_building.jpg`. The following steps need to be implemented.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUvLje_UhPRA"
      },
      "source": [
        "### Problem 2.1 Smoothing [1 pt]\n",
        "First, we need to smooth the images in order to prevent noise from being considered as edges. For this problem, use Gaussian kernel filter with $\\sigma = 1$ to smooth the images. \n",
        "Use the <code>gaussian2d()</code> from Problem 1.1.3 to obtain the Gaussian kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YIzLkpjshPRA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from skimage import io\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.rcParams['figure.figsize'] = [5, 5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "W3kz--8lhPRC"
      },
      "outputs": [],
      "source": [
        "def smooth(image, sig):\n",
        "    \"\"\"\n",
        "    Smoothing the image to prevent the noise from being considered as edges.\n",
        "    Arguments:\n",
        "        image: numpy array of shape (H, W)\n",
        "        sig: Standard deviation of the Gaussian smoothing kernel.\n",
        "    Returns:\n",
        "        smoothed_image: numpy array of shape (H, W)\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\" ==========\n",
        "    YOUR CODE HERE\n",
        "    ========== \"\"\"\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7GwZJbaqZzP",
        "outputId": "e4e49153-4d72-46b9-dafe-4caae3f2d2db"
      },
      "outputs": [],
      "source": [
        "# Load image in grayscale\n",
        "image = io.imread('ucsd_building.jpg', as_gray=True)\n",
        "assert len(image.shape) == 2, 'image must be grayscale; check your Python/skimage versions'\n",
        "smoothed = smooth(image, 1)\n",
        "print('Original:')\n",
        "plt.imshow(image, cmap=cm.gray)\n",
        "plt.show()\n",
        "\n",
        "print('Smoothed:')\n",
        "plt.imshow(smoothed, cmap=cm.gray)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-he-OVGzhPRC"
      },
      "source": [
        "### Problem 2.2 Gradient Computation [5 pts]\n",
        "After you have finished smoothing, find the image gradient in the horizontal and vertical directions. Compute the gradient magnitude image as $|G| = \\sqrt{G_x^2 + G_y^2}$. The edge direction for each pixel is given by $G_\\theta = \\tan^{-1}\\left(\\frac{G_y}{G_x}\\right)$.\n",
        "\n",
        "You must not have any divide by zero errors. Consider using `np.arctan2()`. Use the `conv()` function you defined earlier. **You must use the 3-point central difference kernel**.\n",
        "\n",
        "Show the gradient magnitude and gradient direction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5ZNWuybShPRD"
      },
      "outputs": [],
      "source": [
        "def gradient(image):\n",
        "    \"\"\"\n",
        "    Compute the gradient of the image using the 3-point central difference kernels.\n",
        "    Arguments:\n",
        "        image: numpy array of shape (H, W)\n",
        "    Returns:\n",
        "        g_mag: numpy array of shape (H, W), containing the magnitude of the gradient\n",
        "        g_theta: numpy array of shape (H, W), containing the direction of the gradient\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\" ==========\n",
        "    YOUR CODE HERE\n",
        "    ========== \"\"\"\n",
        "    \n",
        "    return g_mag, g_theta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vo_Yl_kwhPRD",
        "outputId": "32a358c2-17a7-430b-c69d-87372575f0b4"
      },
      "outputs": [],
      "source": [
        "g_mag, g_theta = gradient(smoothed)\n",
        "print('Gradient magnitude:')\n",
        "plt.imshow(g_mag, cmap=cm.gray)\n",
        "plt.show()\n",
        "\n",
        "print('Gradient direction:')\n",
        "plt.imshow(g_theta, cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQaoA2tShPRD"
      },
      "source": [
        "### Problem 2.3 Non-Maximum Suppression [7 pts]\n",
        "We would like our edges to be sharp, unlike the ones in the gradient image. Use non-maximum suppression to preserve all local maxima and discard the rest. You can use the following method to do so:\n",
        "\n",
        "  - For each pixel in the gradient magnitude image:\n",
        "    - Round the gradient direction $\\theta$ to the nearest multiple of $45^{\\circ}$ (which we will refer to as $ve$).\n",
        "    - Compare the edge strength at the current pixel to the pixels along the $ve$ and the opposite direction of $ve$ in the 8-connected neighborhood.\n",
        "    - If the pixel does not have a larger value than both of its two neighbors in the $ve$ and the opposite direction of $ve$, suppress the pixel's value (set it to 0). By following this process, we preserve the values of only those pixels which have maximum gradient magnitudes in the neighborhood along the $ve$ and the opposite direction of $ve$.\n",
        "  - Return the result as the NMS response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "DUcpUj6shPRE"
      },
      "outputs": [],
      "source": [
        "def nms(g_mag, g_theta):\n",
        "    \"\"\"\n",
        "    Non-maximum Suppression - to sharpen our edges\n",
        "\n",
        "    Arguments:\n",
        "        g_mag: numpy array of shape (H, W), containing the magnitude of the gradient\n",
        "        g_theta: numpy array of shape (H, W), containing the direction of the gradient\n",
        "    Returns:\n",
        "        nms_response: numpy array of shape (H, W)\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\" ==========\n",
        "    YOUR CODE HERE\n",
        "    ========== \"\"\"\n",
        "\n",
        "    \n",
        "    return nms_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krS8fp4ChPRE",
        "outputId": "bdccbc76-ef2e-4f15-fcc4-642f3181a472"
      },
      "outputs": [],
      "source": [
        "nms_image = nms(g_mag, g_theta)\n",
        "print('NMS:')\n",
        "plt.imshow(nms_image, cmap=cm.gray)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJNkwzRvhPRF"
      },
      "source": [
        "### Problem 2.4 Hysteresis Thresholding [8 pts]\n",
        "Choose suitable values of thresholds and use the thresholding approach decribed in lecture 6. This will remove the edges caused by noise and color variations.\n",
        "\n",
        "  - Define two thresholds `t_min` and `t_max`.\n",
        "  - If the `nms > t_max`, then we select that pixel as an edge.\n",
        "  - If `nms < t_min`, we reject that pixel.\n",
        "  - If `t_min < nms < t_max`, we select the pixel only if there is a path from/to another pixel with `nms > t_max`. (Hint: Think of all pixels with `nms > t_max` as starting points and run BFS/DFS from these starting points).\n",
        "  - The choice of value of low and high thresholds depends on the range of values in the gradient magnitude image. You can start by setting the high threshold to some percentage of the max value in the gradient magnitude image, e.g. thres_high = 0.2 * image.max(), and the low threshold to some percentage of the high threshold, e.g. thres_low = 0.5 * thres_high. And then you can tune those values however you want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnqI-OxfhPRF"
      },
      "outputs": [],
      "source": [
        "def hysteresis_threshold(image):\n",
        "    \"\"\"\n",
        "    Hysteris Thresholding to remove edges caused by noise and color variations.\n",
        "\n",
        "    Arguments:\n",
        "        image: numpy array of shape (H, W)\n",
        "    Returns:\n",
        "        result: numpy array of shape (H, W)\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\" ==========\n",
        "    YOUR CODE HERE\n",
        "    ========== \"\"\"\n",
        "    # Perform thresholding\n",
        "    \n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQfkYw8bhPRG",
        "outputId": "c09b4311-af4c-4a6a-f811-232b6261f626"
      },
      "outputs": [],
      "source": [
        "thresholded = hysteresis_threshold(nms_image)\n",
        "print('Thresholded:')\n",
        "plt.imshow(thresholded, cmap=cm.gray)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S95eauI0Wy0"
      },
      "source": [
        "## Problem 3 Corner detection [20 pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bwHmTqjqZzQ"
      },
      "source": [
        "### Problem 3.1 [19 pts]\n",
        "In this problem, we are going to build a corner detector. This must be done according to the lecture slides. You must fill in the function <code>corner_detect</code> below, which takes as input <code>image</code>, <code>nCorners</code>, <code>smoothSTD</code>, <code>windowSize</code> -- where `smoothSTD` is the standard deviation of the smoothing kernel and `windowSize` is the window size for corner detector and non-maximum suppression. In the lecture, the corner detector was implemented using a hard threshold. Do not do that; instead, return the `nCorners` strongest corners after non-maximum suppression. This way you can control exactly how many corners are returned.\n",
        "\n",
        "We will be using three different images for this question: dino, matrix, warrior. There are two images of each in their respective folders.\n",
        "\n",
        "In this problem, try the following different standard deviation $\\sigma$ parameters for the Gausian smoothing kernel: 0.8, 1.5, 2.5, 3.5 and 4.5. For a particular $\\sigma$, you must take the kernel size to be $6\\sigma$. Add $1$ if the kernel size is even. So, for example, if $\\sigma=2$, corner detection kernel size is $13$. This must be followed throughout all of the experiments in this assignment.\n",
        "\n",
        "Run your code on all images (with `nCorners` = 20) and display outputs as shown below. There will be a total of 30 images as outputs: 5 choices of `smoothSTD` x (2 `dino` + 2 `matrix` + 2 `warrior` images).\n",
        "\n",
        "Note: You may find `scipy.ndimage.gaussian_filter` helpful for smoothing. You may use the library function `scipy.signal.convolve()` for convolution purposes in this question. These functions are highly optimized and can speed up your code. **Do not use library functions that directly compute the eigenvalues. You must implement the equation for the minor eigenvalue of a 2x2 matrix provided in lecture.**\n",
        "\n",
        "<!--- ![dinoCorner1](dinoCorner1.png) --->\n",
        "<!--- The previous results in export to pdf errors on some systems but the following does not --->\n",
        "<img src=\"dinoCorner1.png\">\n",
        "<!--- ![dinoCorner2](dinoCorner2.png) --->\n",
        "<!--- The previous results in export to pdf errors on some systems but the following does not --->\n",
        "<img src=\"dinoCorner2.png\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "zHf0vo2UhPRG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter\n",
        "import imageio.v2 as imageio\n",
        "from scipy.signal import convolve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Ho-5K4ccqZzQ"
      },
      "outputs": [],
      "source": [
        "def rgb2gray(rgb):\n",
        "    \"\"\"\n",
        "    Convert rgb image to grayscale.\n",
        "    \"\"\"\n",
        "    return np.dot(rgb[...,:3], [0.21263903, 0.71516871, 0.072192319])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "H29Q-x8ohPRH"
      },
      "outputs": [],
      "source": [
        "def corner_detect(image, nCorners, smoothSTD, windowSize):\n",
        "    \"\"\"\n",
        "    Detect corners on a given image.\n",
        "\n",
        "    Args:\n",
        "        image: Given a grayscale image on which to detect corners.\n",
        "        nCorners: Total number of corners to be extracted.\n",
        "        smoothSTD: Standard deviation of the Gaussian smoothing kernel.\n",
        "        windowSize: Window size for corner detector and non-maximum suppression.\n",
        "\n",
        "    Returns:\n",
        "        Detected corners (in image coordinate) in a numpy array (n*2).\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\" ==========\n",
        "    YOUR CODE HERE\n",
        "    ========== \"\"\"\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UetPWSzQhPRI",
        "outputId": "6660229b-7b14-4029-8757-e21d12b044cc"
      },
      "outputs": [],
      "source": [
        "def show_corners_result(imgs, corners):\n",
        "    fig = plt.figure(figsize=(8, 8))\n",
        "    ax1 = fig.add_subplot(221)\n",
        "    ax1.imshow(imgs[0], cmap='gray')\n",
        "    ax1.scatter(corners[0][:, 0], corners[0][:, 1], s=35, edgecolors='r', facecolors='none')\n",
        "\n",
        "    ax2 = fig.add_subplot(222)\n",
        "    ax2.imshow(imgs[1], cmap='gray')\n",
        "    ax2.scatter(corners[1][:, 0], corners[1][:, 1], s=35, edgecolors='r', facecolors='none')\n",
        "    plt.show()\n",
        "\n",
        "for smoothSTD in (0.8, 1.5, 2.5, 3.5, 4.5):\n",
        "    windowSize = int(smoothSTD * 6)\n",
        "    if windowSize % 2 == 0:\n",
        "        windowSize += 1\n",
        "\n",
        "    print('smooth stdev: %r' % smoothSTD)\n",
        "    print('window size: %r' % windowSize)\n",
        "\n",
        "    nCorners = 20\n",
        "\n",
        "    # read images and detect corners on images\n",
        "\n",
        "    imgs_din = []\n",
        "    crns_din = []\n",
        "    imgs_mat = []\n",
        "    crns_mat = []\n",
        "    imgs_war = []\n",
        "    crns_war = []\n",
        "\n",
        "    for i in range(2):\n",
        "        img_din = imageio.imread('dino/dino' + str(i) + '.png')\n",
        "        imgs_din.append(rgb2gray(img_din))\n",
        "        # downsize your image in case corner_detect runs slow in test\n",
        "        # imgs_din.append(rgb2gray(img_din)[::2, ::2])\n",
        "        crns_din.append(corner_detect(imgs_din[i], nCorners, smoothSTD, windowSize))\n",
        "\n",
        "        img_mat = imageio.imread('matrix/matrix' + str(i) + '.png')\n",
        "        imgs_mat.append(rgb2gray(img_mat))\n",
        "        # downsize your image in case corner_detect runs slow in test\n",
        "        # imgs_mat.append(rgb2gray(img_mat)[::2, ::2])\n",
        "        crns_mat.append(corner_detect(imgs_mat[i], nCorners, smoothSTD, windowSize))\n",
        "\n",
        "        img_war = imageio.imread('warrior/warrior' + str(i) + '.png')\n",
        "        imgs_war.append(rgb2gray(img_war))\n",
        "        # downsize your image in case corner_detect runs slow in test\n",
        "        # imgs_war.append(rgb2gray(img_war)[::2, ::2])\n",
        "        crns_war.append(corner_detect(imgs_war[i], nCorners, smoothSTD, windowSize))\n",
        "\n",
        "    show_corners_result(imgs_din, crns_din)\n",
        "    show_corners_result(imgs_mat, crns_mat)\n",
        "    show_corners_result(imgs_war, crns_war)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH7j4FPMhPRI"
      },
      "source": [
        "### Problem 3.2 [1 pts]\n",
        "Comment on your results and observations. You don't need to comment per output; just discuss any trends you see for the detected corners as you change the windowSize and increase the smoothing with respect to the three pairs of images (warrior, dino and matrix). Also discuss whether you are able to find corresponding corners for the pairs of images.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_KyitqWhPRY"
      },
      "source": [
        "## Problem 4 Epipolar rectification and feature matching [60 pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rh2IH6EJqZzU"
      },
      "source": [
        "### 4.1 Theory: Computing the Essential Matrix [6 pts]\n",
        "\n",
        "Consider a stereo rig, formed of two cameras. The rotation and translation of each camera is given to you. Compute the essential matrix $\\mathtt{E}$ from the given information.\n",
        "\n",
        "Note: You may use Numpy library to compute the essential matrix. Show the code for computation and the result.\n",
        "\n",
        "Camera 1:\n",
        "\n",
        "$$ R_1 =\n",
        "\\begin{bmatrix}\n",
        "1 & 0 & 0 \\\\\n",
        "0 & -\\frac{\\sqrt{3}}{2} & - \\frac{1}{3} \\\\\n",
        "0 & \\frac{1}{3} & -\\frac{\\sqrt{3}}{2} \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$ t_1 =\n",
        "\\begin{bmatrix}\n",
        "4 \\\\ 3 \\\\ 1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Camera 2:\n",
        "$$ R_2 =\n",
        "\\begin{bmatrix}\n",
        "1 & 0 & 0 \\\\\n",
        "0 & \\frac{1}{\\sqrt{2}} & - \\frac{1}{2} \\\\\n",
        "0 & \\frac{1}{2} & \\frac{{1}}{\\sqrt{2}} \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$ t_2 =\n",
        "\\begin{bmatrix}\n",
        "-4 \\\\ 3 \\\\ 1\n",
        "\\end{bmatrix}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" ==========\n",
        "YOUR CODE HERE TO COMPUTE AND PRINT THE ESSENTIAL MATRIX\n",
        "========== \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6c-VxCYhPRY"
      },
      "source": [
        "### 4.2 Epipolar rectification [22 pts]\n",
        "\n",
        "In this problem, we are going to perform epipolar rectification. Given calibrated stereo cameras (i.e., calibration matrices $K_1$ and $K_2$, camera rotation matrices $R_1$ and $R_2$, camera translation vectors $t_1$ and $t_2$), you are expected to determine the rotation matrix $R$ and calibration matrix $K$ of the virtual cameras.\n",
        "Your goal is to complete the function <code>epipolarRecification</code>, which determines the calibration matrix and rotation matrix of both cameras, the translation vector of each of the cameras, and matching planar transformations that epipolar rectify the two images acquired by the cameras. The virtual cameras have the same centers as the source real cameras.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuE4P7OwhPRZ"
      },
      "source": [
        "#### 4.2.1 Camera translation matrices and Projective Transformation matrices [6 pts]\n",
        "To calculate the camera translation from cameras with the same camera center, you will have to complete the <code>cameraTranslation</code> first.\n",
        "Another function you need to complete is <code>calcProjectiveTransformation</code>, which calculates the planar projective transformation from cameras with the same camera center.\n",
        "The camera calibration matrix (same for both cameras) will be calculated by <code>calcVirtualK</code>. This is provided for you.\n",
        "To get the rotation matrix $R$ of the virtual camera, we usually interpolate halfway between the two 3D rotations embodied by $R_1$ and $R_2$. For simplicity, this will be also given to you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ghuuFXtmhPRZ"
      },
      "outputs": [],
      "source": [
        "import imageio.v2 as imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "import pickle\n",
        "from math import floor, ceil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Lqrd1nwzhPRa"
      },
      "outputs": [],
      "source": [
        "def cameraTranslation(R_real, t_real, R_virt):\n",
        "    '''\n",
        "    Calculate the camera translation of virtual camera from real camera with the same camera center.\n",
        "\n",
        "    Args:\n",
        "    R_real: The rotation matrix of the real camera.\n",
        "    t_real: The translation vector of the real camera.\n",
        "    R_virt: The rotation matrix of the virtual camera.\n",
        "\n",
        "    Returns:\n",
        "    The translation vector of the virtual camera.\n",
        "    '''\n",
        "    \"\"\" ==========\n",
        "    YOUR CODE HERE\n",
        "    ========== \"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "iyzPCn4qhPRa"
      },
      "outputs": [],
      "source": [
        "def calcProjectiveTransformation(K_real, R_real, K_virt, R_virt):\n",
        "    '''\n",
        "    Calculates the planar projective transformation from cameras with the same camera center.\n",
        "    This function determines the planar projective transformation from the image of a 3D point in the real camera to its image in the virtual camera\n",
        "    where P_real = K_real * R_real * [I | -C] and P_virt = K_virt * R_virt * [I | -C].\n",
        "\n",
        "    Args:\n",
        "    K_real: The calibration matrix of the real camera.\n",
        "    R_real: The rotation matrix of the real camera.\n",
        "    K_virt: The calibration matrix of the virtual camera.\n",
        "    R_virt: The rotation matrix of the virtual camera.\n",
        "\n",
        "    Returns:\n",
        "    The transformation matrix.\n",
        "    '''\n",
        "    \"\"\" ==========\n",
        "    YOUR CODE HERE\n",
        "    ========== \"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eMmC99vhPRb"
      },
      "outputs": [],
      "source": [
        "def calcVirtualK(K1_real, K2_real):\n",
        "    '''\n",
        "    Camera calibration matrix (same for both cameras)\n",
        "    '''\n",
        "    alpha = (K1_real[0][0] + K2_real[0][0] + K1_real[1][1] + K2_real[1][1]) // 4\n",
        "    x0 = (K1_real[0][2] + K2_real[0][2]) // 2\n",
        "    y0 = (K1_real[1][2] + K2_real[1][2]) // 2\n",
        "    K_virt = np.zeros((3, 3))\n",
        "    K_virt[0][0] = alpha\n",
        "    K_virt[0][2] = x0\n",
        "    K_virt[1][1] = alpha\n",
        "    K_virt[1][2] = y0\n",
        "    K_virt[2][2] = 1\n",
        "    return K_virt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0iGdHjwhPRc"
      },
      "outputs": [],
      "source": [
        "import scipy.linalg\n",
        "import math\n",
        "\n",
        "def calcVirtualR(R1_real, R2_real, t1_real, t2_real):\n",
        "    '''\n",
        "    interpolate between two rotation matrices\n",
        "    '''\n",
        "\n",
        "    # Rotation matrix that is half way between R1_real and R2_real\n",
        "    Rinterp = scipy.linalg.expm(0.5*scipy.linalg.logm(R2_real@R1_real.T))@R1_real\n",
        "\n",
        "    # Rotation matrix to compose with above rotation matrix such that relative camera translation vector is aligned with the X-axis\n",
        "    u = cameraTranslation( R2_real, t2_real, Rinterp ) - cameraTranslation( R1_real, t1_real, Rinterp)\n",
        "    vhat = np.array([[1],[0],[0]])\n",
        "\n",
        "    if 0 > u.T@vhat:\n",
        "        #Unit vector along negative X-axis instead, so that the images are not upside down\n",
        "        vhat[0] = -1\n",
        "\n",
        "    # The 3-vector 'axis' defines an axis and theta is the rotation about the axis.\n",
        "    theta =  math.acos((u.T@vhat)[0,0]/np.linalg.norm(u))\n",
        "    axis = np.cross(u.reshape(-1),vhat.reshape(-1))\n",
        "\n",
        "    # The angle-axis representation is a 3-vector omega where the norm of omega is theta and the unitized omega is the unit vector representing the axis of rotation.\n",
        "    omega = (theta/np.linalg.norm(axis))*axis\n",
        "    omega = omega.reshape(-1)\n",
        "\n",
        "    # omega_x is the skew symmetric matrix form of omega\n",
        "    omega_x  = np.array([[0, -omega[2], omega[1]],\n",
        "                         [omega[2], 0, -omega[0]],\n",
        "                         [-omega[1], omega[0], 0]])\n",
        "\n",
        "\n",
        "    R_x = scipy.linalg.expm(omega_x)\n",
        "    R_virt = R_x@Rinterp\n",
        "\n",
        "    return R_virt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zS0pQ1i_hPRc"
      },
      "outputs": [],
      "source": [
        "def epipolarRecification(K1_real, R1_real, t1_real,\n",
        "                         K2_real, R2_real, t2_real):\n",
        "    '''\n",
        "    Given two calibrated cameras, this function determines the calibration matrix and rotation matrix of both cameras, the translation vector of each of the cameras, and matching planar transformations that epipolar rectify the two image acquired by the cameras.\n",
        "    The virtual cameras have the same centers as the source cameras.\n",
        "\n",
        "    Args:\n",
        "    K1_real: The calibration matrix of the first source camera.\n",
        "    R1_real: The rotation matrix of the first source camera.\n",
        "    t1_real: The translation vector of the first source camera.\n",
        "    K2_real: The calibration matrix of the second source camera.\n",
        "    R2_real: The rotation matrix of the second source camera.\n",
        "    t2_real: The translation vector of the second source camera.\n",
        "\n",
        "    Returns:\n",
        "    K_virt: The calibration matrix of the virtual cameras.\n",
        "    t1_virt: The translation vector of the first virtual camera.\n",
        "    t2_virt: The translation vector of the second virtual camera.\n",
        "    H1, H2: The image rectification transformation matrices.\n",
        "    '''\n",
        "    R_virt = calcVirtualR(R1_real, R2_real, t1_real, t2_real)\n",
        "\n",
        "    t1_virt = cameraTranslation(R1_real, t1_real, R_virt)\n",
        "    t2_virt = cameraTranslation(R2_real, t2_real, R_virt)\n",
        "\n",
        "    K_virt = calcVirtualK(K1_real, K2_real)\n",
        "\n",
        "    H1 = calcProjectiveTransformation(K1_real, R1_real, K_virt, R_virt)\n",
        "    H2 = calcProjectiveTransformation(K2_real, R2_real, K_virt, R_virt)\n",
        "\n",
        "    return K_virt, t1_virt, t2_virt, H1, H2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgWg2qzThPRd"
      },
      "source": [
        "#### Problem 4.2.2 Warp Image [10 pts]\n",
        "After calling <code>epipolarRectification</code>, we can get the projective transformation matrices $H1$ and $H2$. Next, we will geometrically transform (i.e., \"warp\") the image so that the epipolar lines are image rows. You must complete <code>warpImage</code> using the **backward** method in Lecture 7. Note the virtual camera images are required to be the same size as the source images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "gp0RswKahPRd"
      },
      "outputs": [],
      "source": [
        "def warpImage(image, H, out_height, out_width):\n",
        "    \"\"\"\n",
        "    Performs the warp of the full image content.\n",
        "    Calculates bounding box by piping four corners through the transformation.\n",
        "\n",
        "    Args:\n",
        "    image: Image to warp\n",
        "    H: The image rectification transformation matrices.\n",
        "    out_height, out_width: The shape of the output image.\n",
        "\n",
        "    Returns:\n",
        "    Out: An inverse warp of the image, given a homography.\n",
        "    min_x, min_y, max_x, max_y: The minimum/maximum of the warped image bound.\n",
        "                                Determine them with the forward method applied to the corners of the source image.\n",
        "    \"\"\"\n",
        "    \"\"\" ==========\n",
        "    YOUR CODE HERE\n",
        "    ========== \"\"\"\n",
        "    \n",
        "    return out, min_x, min_y, max_x, max_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYX7pIYghPRd"
      },
      "outputs": [],
      "source": [
        "file_param = open('data.pkl', 'rb')\n",
        "param = pickle.load(file_param)\n",
        "file_param.close()\n",
        "K1_real, R1_real, t1_real = param['K1_real'], param['R1_real'], param['t1_real']\n",
        "K2_real, R2_real, t2_real = param['K2_real'], param['R2_real'], param['t2_real']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60-AOeZVhPRe",
        "outputId": "d2d0e464-d075-49c5-e164-264293188c40"
      },
      "outputs": [],
      "source": [
        "K_virt, t1_virt, t2_virt, H1, H2 = epipolarRecification(K1_real, R1_real, t1_real,\n",
        "                                                    K2_real, R2_real, t2_real)\n",
        "\n",
        "#Read Images\n",
        "src1 = imageio.imread('BalMire0_OG0.bmp')\n",
        "src2 = imageio.imread('BalMire1_OG0.bmp')\n",
        "\n",
        "#Inverse Warp Image 1\n",
        "height1, width1, _ = src1.shape\n",
        "rectified_im1_unbounded, min_x1, min_y1, max_x1, max_y1 = warpImage(src1, H1, height1, width1)\n",
        "\n",
        "#Inverse Warp Image 2\n",
        "height2, width2, _ = src2.shape\n",
        "rectified_im2_unbounded, min_x2, min_y2, max_x2, max_y2 = warpImage(src2, H2, height2, width2)\n",
        "\n",
        "# Plot the 4 images (2 original and 2 unbounded rectified)\n",
        "fig, ax = plt.subplots(2, 2, figsize=(12, 12))\n",
        "ax[0][0].imshow(src1)\n",
        "ax[0][0].set_title('Original Image 1')\n",
        "ax[0][1].imshow(src2)\n",
        "ax[0][1].set_title('Original Image 2')\n",
        "ax[1][0].imshow(rectified_im1_unbounded)\n",
        "ax[1][0].set_title('Unbounded Rectified Image 1')\n",
        "ax[1][1].imshow(rectified_im2_unbounded)\n",
        "ax[1][1].set_title('Unbounded Rectified Image 2')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hk6xX7sbhPRh"
      },
      "source": [
        "#### 4.2.3 Partial bounded retification [3 pts]\n",
        "In the resulting images, although they are epipolar rectified, you should observe portions of the source images being transformed \"out of bounds\" of the virtual camera images.\n",
        "To fix this problem, we can introduced a 2D transformation containing a translation (i.e., $T1$ and $T2$).\n",
        "$$T1 = \\begin{bmatrix}\n",
        "1 & 0 & -min\\_x1 - 0.5\\\\\n",
        "0 & 1 & -\\min(min\\_y1, min\\_y2) - 0.5\\\\\n",
        "0 & 0 & 1\n",
        "\\end{bmatrix}$$\n",
        "$$T2 = \\begin{bmatrix}\n",
        "1 & 0 & -min\\_x2 - 0.5\\\\\n",
        "0 & 1 & -\\min(min\\_y1, min\\_y2) - 0.5\\\\\n",
        "0 & 0 & 1\n",
        "\\end{bmatrix}$$\n",
        "$H1$, $H2$ can be updated by left multiplying $T1$, $T2$, respectivley. Again, geometrically tranform the images under the updated $H1$, $H2$. The virtual camera image is required to be the same size as the source images. In the resulting images, although they are (still) epipolar rectified, you should observe the portions of the source images being transformed are no longer \"out of bounds\" on the top and left of the partial bound rectified images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPkBCTqqhPRh"
      },
      "outputs": [],
      "source": [
        "def partialboundedRetification(min_x1, min_y1, min_x2, min_y2, H1, H2):\n",
        "    '''\n",
        "    Update the projective transformation matries so that the rectified images are no longer 'out of bound'.\n",
        "\n",
        "    Args:\n",
        "    min_x1, min_y1: The min bounds of warped image 1.\n",
        "    min_x2, min_y2: The min bounds of warped image 2.\n",
        "    H1, H2: The image rectification transformation matrices.\n",
        "\n",
        "    Returns:\n",
        "    H1_bounded, H2_bounded: The updated image rectification transformation matrices.\n",
        "    \n",
        "    '''\n",
        "    \"\"\" ==========\n",
        "    YOUR CODE HERE\n",
        "    ========== \"\"\"\n",
        "\n",
        "    return H1_bounded, H2_bounded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8whZi0qwhPRi",
        "outputId": "2a81712d-af0d-48b3-ba49-e6b7d9b3d602"
      },
      "outputs": [],
      "source": [
        "#Compute the updated H1 and H2 for partial bounded rectification\n",
        "H1_bounded, H2_bounded = partialboundedRetification(min_x1, min_y1, min_x2, min_y2, H1, H2)\n",
        "\n",
        "#Partial Bounded Rectification of Image 1\n",
        "rectified_im1_bounded, min_x1_bounded, min_y1_bounded, max_x1_bounded, max_y1_bounded = warpImage(src1, H1_bounded, height1, width1)\n",
        "\n",
        "#Partial Bounded Rectification of Image 2\n",
        "rectified_im2_bounded, min_x2_bounded, min_y2_bounded, max_x2_bounded, max_y2_bounded = warpImage(src2, H2_bounded, height2, width2)\n",
        "\n",
        "# Plot the 2 partial bounded rectified images\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 12))\n",
        "ax[0].imshow(rectified_im1_bounded)\n",
        "ax[0].set_title('Partial bounded rectified image 1')\n",
        "ax[1].imshow(rectified_im2_bounded)\n",
        "ax[1].set_title('Partial bounded rectified image 2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zpwhuk_lhPRj"
      },
      "source": [
        "#### 4.2.4 Completely bounded rectification [3 pts]\n",
        "Finally, determine the size of the virtual camera images that completely bound the transformed images.\n",
        "\n",
        "\n",
        " $$dst1Width = \\text{int}( max\\_x1 - min\\_x1 + 1 )$$\n",
        " $$dst2Width = \\text{int}( max\\_x2 - min\\_x2 + 1 )$$\n",
        " $$dstHeight = \\text{int}(\\max( max\\_y1, max\\_y2 ) - \\min( min\\_y1, min\\_y2 ) + 1 )$$\n",
        "   \n",
        "   \n",
        " Again, geometrically transform the images under the updated 2D projective transformation matrices $H1$ and $H2$ (these are not updated a second time). You must complete the function <code>completelyBoundedRectification</code>. The fully bounded virtual camera images are required to be the size you just calculated. In the resulting images, you should observe the source images being transformed such that they are epipolar rectified and are completely bounded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVRTop3ohPRj"
      },
      "outputs": [],
      "source": [
        "def completelyBoundedRectification(src1, src2, H1_bounded, H2_bounded, min_x1_bounded, max_x1_bounded,\n",
        "                                   min_y1_bounded, max_y1_bounded, min_x2_bounded, max_x2_bounded,\n",
        "                                   min_y2_bounded, max_y2_bounded):\n",
        "    '''\n",
        "    Determine the size of the virtual camera images (same size for both) that completely bound the transformed images.\n",
        "    Geometrically transform the images under the updated 2D projective transformation matrices H1 and H2 (these are not updated a second time).\n",
        "    Args:\n",
        "    src1, src2: The original images to be rectified.\n",
        "    H1_bounded, H2_bounded: The image rectification transformation matrices after partial bounded rectification.\n",
        "    min_x1_bounded, max_x1_bounded: The min/max x bounds of partially bounded warped image 1.\n",
        "    min_y1_bounded, max_y1_bounded: The min/max y bounds of partially bounded warped image 1.\n",
        "    min_x2_bounded, max_x2_bounded: The min/max x bounds of partially bounded warped image 2.\n",
        "    min_y2_bounded, max_y2_bounded: The min/max y bounds of partially bounded warped image 2.\n",
        "\n",
        "    Returns:\n",
        "    rectified_im1_final, rectified_im2_final: The completely bounded rectified images\n",
        "    \n",
        "    '''\n",
        "    \"\"\" ==========\n",
        "    YOUR CODE HERE\n",
        "    ========== \"\"\"\n",
        "\n",
        "    return rectified_im1_final, rectified_im2_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsqIu875hPRj",
        "outputId": "5e8057e9-ea18-4b1c-fc1d-eb3113062206"
      },
      "outputs": [],
      "source": [
        "rectified_im1_final, rectified_im2_final = completelyBoundedRectification(src1, src2, H1_bounded, H2_bounded, min_x1_bounded, max_x1_bounded,\n",
        "                                   min_y1_bounded, max_y1_bounded, min_x2_bounded, max_x2_bounded, min_y2_bounded, max_y2_bounded)\n",
        "\n",
        "# Plot the 2 completely bounded rectified images\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 12))\n",
        "ax[0].imshow(rectified_im1_final)\n",
        "ax[0].set_title('Completely bounded rectified image 1')\n",
        "ax[1].imshow(rectified_im2_final)\n",
        "ax[1].set_title('Completely bounded rectified image 2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-vuVoJi1loJ"
      },
      "source": [
        "### Problem 4.3 Feature matching [12 pts]\n",
        "#### 4.3.1 SSD (Sum Squared Distance) Matching [2 pts]\n",
        "Complete the function <code>ssdMatch</code>:  \n",
        "$$\\text{SSD} = \\sum_{x,y}|W_1(x,y)-W_2(x,y)|^2$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "9UFeIp94hPRo"
      },
      "outputs": [],
      "source": [
        "def ssdMatch(img1, img2, c1, c2, R):\n",
        "    \"\"\"\n",
        "    Compute SSD given two windows.\n",
        "\n",
        "    Args:\n",
        "        img1: Image 1.\n",
        "        img2: Image 2.\n",
        "        c1: Center (in image coordinate) of the window in image 1.\n",
        "        c2: Center (in image coordinate) of the window in image 2.\n",
        "        R: R is the radius of the patch, 2 * R + 1 is the window size\n",
        "\n",
        "    Returns:\n",
        "        SSD matching score for two input windows.\n",
        "\n",
        "    \"\"\"\n",
        "    \"\"\" ==========\n",
        "    YOUR CODE HERE\n",
        "    ========== \"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZa3MA-XhPRo",
        "outputId": "53b41ab7-f501-4629-ab68-a4f73f130d33"
      },
      "outputs": [],
      "source": [
        "# Here is the code for you to test your implementation\n",
        "img1 = np.array([[1, 3, 5, 2], [2, 8, 7, 7], [4, 2, 3, 0]])\n",
        "img2 = np.array([[4, 1, 5, 4], [4, 6, 2, 5], [2, 7, 9, 4]])\n",
        "print(ssdMatch(img1, img2, np.array([1, 1]), np.array([1, 1]), 1))\n",
        "# should print 111\n",
        "print(ssdMatch(img1, img2, np.array([2, 1]), np.array([2, 1]), 1))\n",
        "# should print 118\n",
        "print(ssdMatch(img1, img2, np.array([1, 1]), np.array([2, 1]), 1))\n",
        "# should print 120"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8zjtA0_hPRp"
      },
      "source": [
        "#### Problem 4.3.2 NCC (Normalized Cross-Correlation) Matching [8 pts]\n",
        "\n",
        "Write a function <code>ncc_match</code> that implements the NCC matching algorithm for two input windows.\n",
        "\n",
        "$$\\text{NCC} = \\sum_{i,j}\\tilde{W_1} (i,j)\\cdot \\tilde{W_2} (i,j)$$\n",
        "\n",
        "where $\\tilde{W} = \\frac{W - \\overline{W}}{\\sqrt{\\sum_{k,l}(W(k,l) - \\overline{W})^2}}$ is a mean-shifted and normalized version of the window and $\\overline{W}$ is the mean pixel value in the window W.\n",
        "\n",
        "Note: Add a small constant (1e-6) to the denominator to ensure there is no divide by 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "d8xYIFpe1rXe"
      },
      "outputs": [],
      "source": [
        "def ncc_match(img1, img2, c1, c2, R):\n",
        "    \"\"\"\n",
        "    Compute NCC given two windows.\n",
        "\n",
        "    Args:\n",
        "        img1: Image 1.\n",
        "        img2: Image 2.\n",
        "        c1: Center (in image coordinate) of the window in image 1.\n",
        "        c2: Center (in image coordinate) of the window in image 2.\n",
        "        R: R is the radius of the patch, 2 * R + 1 is the window size\n",
        "\n",
        "    Returns:\n",
        "        NCC matching score for two input windows.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\" ==========\n",
        "    YOUR CODE HERE\n",
        "    ========== \"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zspXw616hPRq",
        "outputId": "a6f5a7be-600b-4c46-fb34-421bcd69239f"
      },
      "outputs": [],
      "source": [
        "# test NCC match\n",
        "img1 = np.array([[2, 1, 3, 2], [5, 7, 3, 7], [4, 8, 6, 0]])\n",
        "img2 = np.array([[4, 1, 4, 4], [2, 6, 7, 5], [3, 5, 9, 6]])\n",
        "\n",
        "print (ncc_match(img1, img2, np.array([1, 1]), np.array([1, 1]), 1))\n",
        "# should print 0.47509\n",
        "\n",
        "print (ncc_match(img1, img2, np.array([2, 1]), np.array([2, 1]), 1))\n",
        "# should print 0.37888\n",
        "\n",
        "print (ncc_match(img1, img2, np.array([1, 1]), np.array([2, 1]), 1))\n",
        "# should print 0.87091"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbDgv8AEhPRr"
      },
      "source": [
        "#### Problem 4.2.3  [2 pts]\n",
        "\n",
        "**i.  Which feature matching algorithm do you think is better to use between SSD and NCC?**\n",
        "\n",
        "**ii. Give a scenario where your answer in part i would result in better matches.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQwDVXe9qZzV"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "td7AVaC9hPRr"
      },
      "source": [
        "### Problem 4.4 Naive Matching [8 pts]\n",
        "\n",
        "Equipped with the corner detector and the NCC matching function, we are ready to start finding correspondences.  \n",
        "\n",
        "One naive strategy is to try and find the best match between the two sets of corner points. Write a function that does this. For each corner in image1, find the best match from the detected corners in image2 (or, if the NCC match score is too low, then return no match for that point). You will have to figure out a good threshold (<code>NCCth</code>) value by experimentation.\n",
        "\n",
        "Complete the function <code>naive_matching</code> and call it as below. Examine your results for 20, 30, and 35 detected corners in each image. <code>naive_matching</code> will call your NCC matching code.\n",
        "\n",
        "Use $R = 29$, where R is the radius of the NCC patch of size $2\\times R+1$.\n",
        "\n",
        "Complete the function <code>show_matching_result</code> to show the matches. **Properly label or mention which output corresponds to which choice of number of corners. The total number of outputs is 6 images:** (3 choices of number of corners for each of `matrix` and `warrior`), where each figure might look like the following:\n",
        "\n",
        "**Number of corners: 10**\n",
        "<img src = \"dinoMatch.png\" alt=\"dino match\" width=\"800\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "BXxjDfzjhPRt"
      },
      "outputs": [],
      "source": [
        "def naive_matching(img1, img2, corners1, corners2, R, NCCth):\n",
        "    \"\"\"\n",
        "    Naive Matching - find the best match between two sets of corner points.\n",
        "    For each corner in img1, find the best match in corners2 using NCC matching.\n",
        "\n",
        "    Args:\n",
        "        img1: Image 1.\n",
        "        img2: Image 2.\n",
        "        corners1: Corners in image 1 (nx2)\n",
        "        corners2: Corners in image 2 (nx2)\n",
        "        R: NCC matching radius\n",
        "        NCCth: NCC matching score threshold\n",
        "\n",
        "    Returns:\n",
        "        matching:   NCC matching result a list of tuple (c1, c2),\n",
        "                    c1 is the 1x2 corner location in image 1,\n",
        "                    c2 is the 1x2 corner location in image 2.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\" ==========\n",
        "    YOUR CODE HERE\n",
        "    ========== \"\"\"\n",
        "    \n",
        "    return matching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vR7u-sEhPRu"
      },
      "outputs": [],
      "source": [
        "# detect corners on warrior and matrix sets\n",
        "# you are free to modify code here, create your helper functions, etc.\n",
        "\n",
        "nCorners = []\n",
        "smoothSTD = 1\n",
        "windowSize = 7\n",
        "\n",
        "# Read images and detect corners on images\n",
        "\n",
        "imgs_warrior = []\n",
        "crns_warrior = []\n",
        "\n",
        "imgs_matrix = []\n",
        "crns_matrix = []\n",
        "\n",
        "for c in nCorners:\n",
        "    for i in range(2):\n",
        "        img_warrior = imageio.imread('warrior/warrior' + str(i) + '.png')\n",
        "        imgs_warrior.append(rgb2gray(img_warrior))\n",
        "\n",
        "        img_matrix = imageio.imread('matrix/matrix' + str(i) + '.png')\n",
        "        imgs_matrix.append(rgb2gray(img_matrix))\n",
        "\n",
        "        crns_warrior.append(corner_detect(imgs_warrior[i], c, smoothSTD, windowSize))\n",
        "        crns_matrix.append(corner_detect(imgs_matrix[i], c, smoothSTD, windowSize))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb8kec7FhPRv"
      },
      "outputs": [],
      "source": [
        "# match corners\n",
        "R =    # put your radius here\n",
        "NCCth =   # put your threshold here\n",
        "\n",
        "matching_warrior = []\n",
        "matching_matrix = []\n",
        "\n",
        "for i in range(0,6,2):\n",
        "    matching_warrior.append(naive_matching(imgs_warrior[0]/255, imgs_warrior[1]/255, crns_warrior[i], crns_warrior[i+1], R, NCCth))\n",
        "    matching_matrix.append(naive_matching(imgs_matrix[0]/255, imgs_matrix[1]/255, crns_matrix[i], crns_matrix[i+1], R, NCCth))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHD2_5yahPRy",
        "outputId": "575bcd7c-bb3d-4f6c-b762-0df967b91044"
      },
      "outputs": [],
      "source": [
        "# plot matching result\n",
        "def show_matching_result(img1, img2, matching):\n",
        "    \"\"\"\n",
        "    Show the matching result.\n",
        "    Each figure should be similar to the reference image in the instruction.\n",
        "    Arguments:\n",
        "        img1: Image 1.\n",
        "        img2: Image 2.\n",
        "        matching: Matching result a list of tuple (c1, c2),\n",
        "                  c1 is the 1x2 corner location in image 1,\n",
        "                  c2 is the 1x2 corner location in image 2.\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\" ==========\n",
        "    YOUR CODE HERE\n",
        "    ========== \"\"\"\n",
        "    \n",
        "\n",
        "for i,c in enumerate(nCorners):\n",
        "    print(\"Number of Corners:\", c)\n",
        "    show_matching_result(imgs_warrior[0], imgs_warrior[1], matching_warrior[i])\n",
        "    show_matching_result(imgs_matrix[0], imgs_matrix[1], matching_matrix[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPpgSIp7hPRz"
      },
      "source": [
        "### Problem 4.5 Matching using epipolar geometry [10 pts]\n",
        "Next, we will use the epipolar geometry constraint on the rectified images and updated corner points to build a better matching algorithm.\n",
        "\n",
        "1. Detect 25 corners in image1.\n",
        "2. For each corner, do a line search along the corresponding parallel epipolar line in image2.\n",
        "3. Evaluate the NCC score for each point along this line and return the best match (or no match if all scores are below the <code>NCCth</code>).\n",
        "\n",
        "Choose a suitable threshold (<code>NCCth</code>), NCC radius (<code>R</code>) for NCC matching. Also, choose a suitable standard deviation for smoothing (<code>smoothSTD</code>) and window size (<code>windowSize</code>) for corner detection.\n",
        "\n",
        "You do not have to run this in both directions. Show your result as in the naive matching part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zm9a2iFahPRz"
      },
      "outputs": [],
      "source": [
        "def display_correspondence(img1, img2, corrs):\n",
        "    \"\"\"\n",
        "    Plot matching result on image pair given images and correspondences\n",
        "\n",
        "    Args:\n",
        "        img1: Image 1.\n",
        "        img2: Image 2.\n",
        "        corrs: Corner correspondence\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\" ==========\n",
        "    YOUR CODE HERE\n",
        "    You may want to refer to the `show_matching_result` function.\n",
        "    ========== \"\"\"\n",
        "    \n",
        "def correspondence_matching_epipole(img1, img2, corners1, R, NCCth):\n",
        "    \"\"\"\n",
        "    Find corner correspondence along epipolar line.\n",
        "\n",
        "    Args:\n",
        "        img1: Image 1 (Rectified)\n",
        "        img2: Image 2 (Rectified)\n",
        "        corners1: Detected corners in image 1.\n",
        "        R: NCC matching window radius.\n",
        "        NCCth: NCC matching threshold.\n",
        "\n",
        "    Returns:\n",
        "        Matching result to be used in display_correspondence function\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\" ==========\n",
        "    YOUR CODE HERE\n",
        "    ========== \"\"\"\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "TPzaWUE0hPR3"
      },
      "outputs": [],
      "source": [
        "rectified_im1_final, rectified_im2_final = completelyBoundedRectification(src1, src2, H1_bounded, H2_bounded,min_x1_bounded, max_x1_bounded,\n",
        "                                   min_y1_bounded, max_y1_bounded,min_x2_bounded, max_x2_bounded,\n",
        "                                   min_y2_bounded, max_y2_bounded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhNjSkcPhPR4",
        "outputId": "8a037f53-9fab-405e-e5eb-1766bbf8beb4"
      },
      "outputs": [],
      "source": [
        "# replace black pixels with white pixels\n",
        "_black_idxs = (rectified_im1_final[:, :, 0] == 0) & (rectified_im1_final[:, :, 1] == 0) & (rectified_im1_final[:, :, 2] == 0)\n",
        "rectified_im1_final[:, :][_black_idxs] = [1.0, 1.0, 1.0]\n",
        "_black_idxs = (rectified_im2_final[:, :, 0] == 0) & (rectified_im2_final[:, :, 1] == 0) & (rectified_im2_final[:, :, 2] == 0)\n",
        "rectified_im2_final[:, :][_black_idxs] = [1.0, 1.0, 1.0]\n",
        "\n",
        "nCorners = \n",
        "# Choose your threshold and NCC matching window radius\n",
        "NCCth = \n",
        "R = \n",
        "smoothSTD = \n",
        "windowSize = \n",
        "\n",
        "# detect corners using corner detector here, store in corners1\n",
        "corners1 = corner_detect(rgb2gray(rectified_im1_final), nCorners, smoothSTD, windowSize)\n",
        "corrs = correspondence_matching_epipole(rectified_im1_final, rectified_im2_final, corners1, R, NCCth)\n",
        "display_correspondence(rectified_im1_final, rectified_im2_final, corrs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 4.6 Comments [2 pts]\n",
        "\n",
        "Based on your observations, comment on the results of naive matching and matching with epipolar geometry. Discuss about which method outputs accurate results and why? Also discuss why we need to **epipolar rectify** the images?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ECE285_Comp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
